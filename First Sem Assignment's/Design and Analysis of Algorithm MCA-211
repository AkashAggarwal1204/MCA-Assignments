Question 1. a) What are the desirable characteristics of an algorithm? Find the GCD of p = 144 and q = 55 using Euclid’s algorithm.

Answer. 1)Input specified : 
The input is the data to be transformed during the computation to produce the output.An algorithm should have 0 or 
more well-defined inputs.Input precision requires that you know what kind of data, how much and what form the data should be

2)Output specified : 
The output is the data resulting from the computation (your intended result). An algorithm should have 1 or more well-defined
outputs, and should match the desired output.Output precision also requires that you know what kind of data, how much and what form the output should 
be (or even if there will be any output at all!).

3)Definiteness : 
Algorithms must specify every step and the order the steps must be taken in the process.Definiteness means specifying the sequence of 
operations for turning input into output. Algorithm should be clear and unambiguous.Details of each step must be also be spelled out (including how to handle errors).
It should contain everything quantitative and not qualitative.

4)Effectiveness : 
For an algorithm to be effective, it means that all those steps that are required to get to output must be feasible with the available 
resources.It should not contain any unnecessary and redundant steps which could make an algorithm ineffective.

5)Finiteness :
The algorithm must stop, eventually.Stopping may mean that you get the expected output OR you get a response that no solution is possible. Algorithms must terminate
after a finite number of steps.An algorithm should not be infinite and always terminate after definite number of steps. 

There is no point in developing an algorithm which is infinite as it will be useless for us.

6)Independent :
An algorithm should have step-by-step directions, which should be independent of any programming code.It should be such that it could be run on any of the programming languages.
Thus,these are the characteristics that an algorithm should have for its fruitfulness.

--> Find the GCD of p = 144 and q = 55 using Euclid’s algorithm.

GCD of 144 and 55


144 > 55

a = bq + r (0≤r<b)

114  = 55 x 2 + 34
55 = 34 x 1 + 21
34 = 21 x 1 + 13
21 = 13 x 1 + 8
13 = 8 x 1 + 5 
8 = 5 x 1 + 3
5 = 3 x 1 + 2
3 = 2 x 1 + 1
2 = 1 x 1 + 1
1 = 1 x 1 + 0

Here, r = 0
Therefore, Divisor is the GCD
Hence, 1 is the greatest common divisor(GCD) of 144 & 55


--> Find the GCD of p = 144 and q = 55 using Java.

--> First approach

public class GCD {
	
	public static void main(String[] args) {
		int a = 144;
		int b = 55;
		int r;
		do {
			r = a%b;
			a = b;
			b = r;
		} while (r != 0);
		System.out.println(a);
		
		
	}

}

Output = 1


--> Second approach

public class GCD {
	
	 static int gcd(int a, int b)
	    {
	        if (a == 0)
	          return b;
	        if (b == 0)
	          return a;
	      
	        if (a == b)
	            return a;
	        if (a > b)
	            return gcd(a-b, b);
	        return gcd(a, b-a);
	    }
	
	public static void main(String[] args) {
		int p = 114;
		int q = 55;
		System.out.println(gcd(p,q));
	}
	

}

Output = 1



Question 1. b) Differentiate between Greedy Technique and Dynamic Programming approach of problem solving. Name few problems which are solved using these techniques

Answer.
Greedy approach :
A Greedy algorithm is an algorithmic paradigm that builds up a solution piece by piece, always choosing the next piece that offers the most obvious and 
immediate benefit. So the problems where choosing locally optimal also leads to a global solution are best fit for Greedy. For example, consider the Fractional 
Knapsack Problem. The local optimal strategy is to choose the item that has maximum value vs weight ratio. This strategy also leads to global optimal solution
because we allowed taking fractions of an item. 

 Dynamic programming :
 Dynamic programming is mainly an optimization over plain recursion. Wherever we see a recursive solution that has repeated calls for the same inputs, we can optimize
 it using Dynamic Programming. The idea is to simply store the results of subproblems so that we do not have to re-compute them when needed later. This simple
 optimization reduces time complexities from exponential to polynomial. For example, if we write a simple recursive solution for Fibonacci Numbers, we get exponential
 time complexity and if we optimize it by storing solutions of subproblems, time complexity reduces to linear. 
 
 
 Feature	                                                            Greedy method                                         	                                                                                          Dynamic programming
Feasibility	  In a greedy Algorithm, we make whatever choice seems best at the moment in the hope that it will lead to global optimal solution.            In Dynamic Programming we make decision at each step considering current problem and solution to previously solved sub problem to calculate optimal solution .
Optimality	  In Greedy Method, sometimes there is no such guarantee of getting Optimal Solution.	                                                       It is guaranteed that Dynamic Programming will generate an optimal solution as it generally considers all possible cases and then choose the best.
Recursion	  A greedy method follows the problem solving heuristic of making the locally optimal choice at each stage.                                    A Dynamic programming is an algorithmic technique which is usually based on a recurrent formula that uses some previously calculated states.
Memoization	  It is more efficient in terms of memory as it never look back or revise previous choices	                                               It requires dp table for memoization and it increases it’s memory complexity.
Time complexity	  Greedy methods are generally faster. For example, Dijkstra’s shortest path algorithm takes O(ELogV + VLogV) time.	                       Dynamic Programming is generally slower. For example, Bellman Ford algorithm takes O(VE) time.
Fashion	          The greedy method computes its solution by making its choices in a serial forward fashion, never looking back or revising previous choices.  Dynamic programming computes its solution bottom up or top down by synthesizing them from smaller optimal sub solutions.






Question 2. a) Prove that, for all positive integers n, 1 + 2 + 4 + ⋯ + 2! = 2!"# − 1

Answer.
Consider the given statement
P(n):1+2+4+...2ⁿ=2ⁿ⁺¹−1, for natural numbers n.
Step I We observe that P(0) is true.
P(2):1+2+4=8−1
7=7, which is true.
Step II Now, assume that P(n) is true for n=k.
So, P(k) : 1+2+4+...2ᵏ=2ᵏ⁺¹−1 is true.
Step III Now, to prove P(k+1) is true.
P(k+1):1+2+4+...+2ᵏ+2ᵏ⁺¹
=2ᵏ⁺¹−1+2ᵏ⁺¹
=2⋅2ᵏ⁺¹−1
=2ᵏ⁺¹+1−1
So, P(k+1) is true, whenever P(k) is true.
Hence, P(n) is true.


Question 2. b) What are asymptotic notations? Explain the significance of Big- O, Omega and theta notations with suitable example.
Answer. Asymptotic notations are the mathematical notations used to describe the running time of an algorithm when the input tends towards 
a particular value ora limiting value.

For example: 
In bubble sort, when the input array is already sorted, the time taken by the algorithm is linear i.e. the best case.
But, when the input array is in reverse condition, the algorithm takes the maximum time (quadratic) to sort the elements i.e. the worst case.
When the input array is neither sorted nor in reverse order, then it takes average time. These durations are denoted using asymptotic notations.
There are mainly three asymptotic notations:

Big-O notation
Omega notation
Theta notation

Big-O Notation (O-notation)
Big-O notation represents the upper bound of the running time of an algorithm. Thus, it gives the worst-case complexity of an algorithm.

https://cdn.programiz.com/sites/tutorial2program/files/big0.png

O(g(n)) = { f(n): there exist positive constants c and n0
            such that 0 ≤ f(n) ≤ cg(n) for all n ≥ n0 }
The above expression can be described as a function f(n) belongs to the set O(g(n)) if there exists a positive constant c such that it lies between 0 and cg(n),
for sufficiently large n. For any value of n, the running time of an algorithm does not cross the time provided by O(g(n)).
Since it gives the worst-case running time of an algorithm, it is widely used to analyze an algorithm as we are always interested in the worst-case scenario.


Omega Notation (Ω-notation)
Omega notation represents the lower bound of the running time of an algorithm. Thus, it provides the best case complexity of an algorithm.

https://cdn.programiz.com/sites/tutorial2program/files/omega.png

Ω(g(n)) = { f(n): there exist positive constants c and n0 
            such that 0 ≤ cg(n) ≤ f(n) for all n ≥ n0 }
The above expression can be described as a function f(n) belongs to the set Ω(g(n)) if there exists a positive constant c such that it lies above cg(n), 
for sufficiently large n. For any value of n, the minimum time required by the algorithm is given by Omega Ω(g(n)).

Theta Notation (Θ-notation)
Theta notation encloses the function from above and below. Since it represents the upper and the lower bound of the running time of an algorithm,
it is used for analyzing the average-case complexity of an algorithm.

https://cdn.programiz.com/sites/tutorial2program/files/theta.png

For a function g(n), Θ(g(n)) is given by the relation:

Θ(g(n)) = { f(n): there exist positive constants c1, c2 and n0
            such that 0 ≤ c1g(n) ≤ f(n) ≤ c2g(n) for all n ≥ n0 }
The above expression can be described as a function f(n) belongs to the set Θ(g(n)) if there exist positive constants c1 and c2 such that it can be
sandwiched between c1g(n) and c2g(n), for sufficiently large n. If a function f(n) lies anywhere in between c1g(n) and c2g(n) for all n ≥ n0, then f(n) is 
said to be asymptotically tight bound.



